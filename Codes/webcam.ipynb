{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import zipfile\n",
    "from typing import Tuple, Dict, List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from PIL import Image, ImageEnhance\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, Dict, List\n",
    "from torch.utils.data import DataLoader\n",
    "import shutil\n",
    "import splitfolders\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "#from models import GiMeFive\n",
    "#from hook import Hook\n",
    "from pytorch_grad_cam import GradCAM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import dlib\n",
    "import argparse\n",
    "import textwrap\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Hook():\n",
    "    def __init__(self):\n",
    "        self.hook_forward = None\n",
    "        self.hook_backward = None\n",
    "        self.forward_out = None\n",
    "        self.backward_out = None\n",
    "\n",
    "    def hook_fn_forward(self, module, input, output):\n",
    "        self.forward_out = output\n",
    "\n",
    "    def hook_fn_backward(self, module, grad_input, grad_output):\n",
    "        self.backward_out = grad_output[0] \n",
    "\n",
    "    def register_hook(self, module):\n",
    "        self.hook_forward = module.register_forward_hook(self.hook_fn_forward)\n",
    "        self.hook_backward = module.register_full_backward_hook(self.hook_fn_backward)\n",
    "\n",
    "    def unregister_hook(self):\n",
    "        self.hook_forward.remove()\n",
    "        self.hook_backward.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmotionsDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionsDetector, self).__init__()\n",
    "\n",
    "        # 1st Convolutional layer\n",
    "        self.conv1_1 = nn.Conv2d(3, 48, kernel_size=(3, 3), padding=1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(48)\n",
    "        self.conv1_2 = nn.Conv2d(48, 48, kernel_size=(3, 3), padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(48)\n",
    "        self.conv1_3 = nn.Conv2d(48, 48, kernel_size=(3, 3), padding=1)\n",
    "        self.bn1_3 = nn.BatchNorm2d(48)\n",
    "\n",
    "\n",
    "        # 2nd Convolutional layer\n",
    "        self.conv2_1 = nn.Conv2d(48, 64, kernel_size=(3, 3), padding=1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(64)\n",
    "        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=(3, 3), padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(64)\n",
    "        self.conv2_3 = nn.Conv2d(64, 64, kernel_size=(3, 3), padding=1)\n",
    "        self.bn2_3 = nn.BatchNorm2d(64)\n",
    "\n",
    "\n",
    "        # 3rd Convolutional layer\n",
    "        self.conv3_1 = nn.Conv2d(64, 80, kernel_size=(3, 3), padding=1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(80)\n",
    "        self.conv3_2 = nn.Conv2d(80, 80, kernel_size=(3, 3), padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(80)\n",
    "        self.conv3_3 = nn.Conv2d(80, 80, kernel_size=(3, 3), padding=1)\n",
    "        self.bn3_3 = nn.BatchNorm2d(80)\n",
    "\n",
    "\n",
    "        # 4th Convolutional layer\n",
    "        self.conv4_1 = nn.Conv2d(80, 126, kernel_size=(3, 3), padding=1)\n",
    "        self.bn4_1 = nn.BatchNorm2d(126)\n",
    "        self.conv4_2 = nn.Conv2d(126, 126, kernel_size=(3, 3), padding=1)\n",
    "        self.bn4_2 = nn.BatchNorm2d(126)\n",
    "        self.conv4_3 = nn.Conv2d(126, 126, kernel_size=(3, 3), padding=1)\n",
    "        self.bn4_3 = nn.BatchNorm2d(126)\n",
    "        \n",
    "\n",
    "        # pool layers\n",
    "        self.pool1 = nn.MaxPool2d(3, 1)\n",
    "        self.pool2 = nn.MaxPool2d(3, 2)\n",
    "        self.pool3 = nn.MaxPool2d(3, 3)\n",
    "        self.gapool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Linear(126, 6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.bn1_1(self.conv1_1(x)))\n",
    "        x = F.relu(self.bn1_2(self.conv1_2(x)))\n",
    "        x = F.relu(self.bn1_3(self.conv1_3(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.bn2_1(self.conv2_1(x)))\n",
    "        x = F.relu(self.bn2_2(self.conv2_2(x)))\n",
    "        x = F.relu(self.bn2_3(self.conv2_3(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.bn3_1(self.conv3_1(x)))\n",
    "        x = F.relu(self.bn3_2(self.conv3_2(x)))\n",
    "        x = F.relu(self.bn3_3(self.conv3_3(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = F.relu(self.bn4_1(self.conv4_1(x)))\n",
    "        x = F.relu(self.bn4_2(self.conv4_2(x)))\n",
    "        x = F.relu(self.bn4_3(self.conv4_3(x)))\n",
    "        x = self.gapool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from myModel import EmotionsDetector002\n",
    "\n",
    "# load best model\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "class_labels = ['anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise']\n",
    "\n",
    "model = None\n",
    "hook = None\n",
    "transform = None\n",
    "device = None\n",
    "\n",
    "model = EmotionsDetector().to(device)\n",
    "model.load_state_dict(torch.load(\"C:/LMU/python/SEP_DLCV/Final_project/last_work/models/emotionsDetector_state_dict002.pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "final_layer = model.bn4_3\n",
    "hook = Hook()\n",
    "hook.register_hook(final_layer)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def classify_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    image_array = np.array(image)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "    scores = probabilities.cpu().numpy().flatten()\n",
    "    rounded_scores = [round(score, 2) for score in scores]\n",
    "    \n",
    "    return rounded_scores, image, image_array, image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 176\u001b[0m\n\u001b[0;32m    172\u001b[0m     out\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    173\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m--> 176\u001b[0m \u001b[43mevaluate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcamera\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_path_to_video\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m#evaluate_input(source='video', input_path_to_video=\"C:/LMU/python/SEP_DLCV/Final_project/last_work/models/demoo.mp4\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 159\u001b[0m, in \u001b[0;36mevaluate_input\u001b[1;34m(source, input_path_to_video)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# terminate the loop if the frame is not read successfully\u001b[39;00m\n\u001b[0;32m    157\u001b[0m faces \u001b[38;5;241m=\u001b[39m detect_bounding_box(video_frame, counter)  \u001b[38;5;66;03m# apply the function we created to the video frame, faces as variable not used\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMy Face Detection Project\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_frame\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# display the processed frame in a window named \"My Face Detection Project\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m out\u001b[38;5;241m.\u001b[39mwrite(video_frame)  \u001b[38;5;66;03m# write the processed frame to the output video file\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "# OpenCV Real-Time Face Detection, dlib Landmarks, saving camera / video\n",
    "## class_labels = ['happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear']\n",
    "\n",
    "from pathlib import Path\n",
    "face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "predictor = dlib.shape_predictor('C:/LMU/python/SEP-CVDL-main/SEP-CVDL-main/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# text settings\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 1\n",
    "font_color = (154, 1, 254) # BGR color neon pink 254,1,154\n",
    "thickness = 2\n",
    "line_type = cv2.LINE_AA\n",
    "\n",
    "max_emotion = ''\n",
    "transparency = 0.4\n",
    "\n",
    "def detect_emotion(pil_crop_img):\n",
    "    # Convert NumPy array to PIL Image\n",
    "    pil_crop_img = Image.fromarray(pil_crop_img)\n",
    "    \n",
    "    vid_fr_tensor = transform(pil_crop_img).unsqueeze(0).to(device)\n",
    "    # with torch.no_grad():\n",
    "    logits = model(vid_fr_tensor)\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "    predicted_class_idx = predicted_class.item()\n",
    "\n",
    "    one_hot_output = torch.FloatTensor(1, probabilities.shape[1]).zero_()\n",
    "    one_hot_output[0][predicted_class_idx] = 1\n",
    "    logits.backward(one_hot_output, retain_graph=True)\n",
    "\n",
    "    gradients = hook.backward_out\n",
    "    feature_maps = hook.forward_out\n",
    "\n",
    "    weights = torch.mean(gradients, dim=[2, 3], keepdim=True)\n",
    "    cam = torch.sum(weights * feature_maps, dim=1, keepdim=True)\n",
    "    cam = cam.clamp(min=0).squeeze() \n",
    "\n",
    "    cam -= cam.min()\n",
    "    cam /= cam.max()\n",
    "    cam = cam.cpu().detach().numpy()\n",
    "\n",
    "    # scores = probabilities.cpu().numpy().flatten()\n",
    "    scores = probabilities.cpu().detach().numpy().flatten()\n",
    "    rounded_scores = [round(score, 2) for score in scores]\n",
    "    return rounded_scores, cam\n",
    "\n",
    "def plot_heatmap(x, y, w, h, cam, pil_crop_img, video_frame):\n",
    "    # resize cam to w, h\n",
    "    cam = cv2.resize(cam, (w, h))\n",
    "    \n",
    "    # apply color map to resized cam\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    \n",
    "    # Get the region of interest on the video frame\n",
    "    roi = video_frame[y:y+h, x:x+w, :]\n",
    "\n",
    "    # Blend the heatmap with the ROI\n",
    "    overlay = heatmap * transparency + roi / 255 * (1 - transparency)\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "\n",
    "    # Replace the ROI with the blended overlay\n",
    "    video_frame[y:y+h, x:x+w, :] = np.uint8(255 * overlay)\n",
    "        \n",
    "def update_max_emotion(rounded_scores):  \n",
    "    # get index from max value in rounded_scores\n",
    "    max_index = np.argmax(rounded_scores)\n",
    "    max_emotion = class_labels[max_index]\n",
    "    return max_emotion # returns max_emotion as string\n",
    "\n",
    "def print_max_emotion(x, y, max_emotion, video_frame):\n",
    "    # position to put the text for the max emotion\n",
    "    org = (x, y - 15)\n",
    "    cv2.putText(video_frame, max_emotion, org, font, font_scale, font_color, thickness, line_type)\n",
    "    \n",
    "def print_all_emotion(x, y, w, rounded_scores, video_frame):\n",
    "    # create text to be displayed\n",
    "    org = (x + w + 10, y - 20)\n",
    "    for index, value in enumerate(class_labels):\n",
    "        emotion_str = (f'{value}: {rounded_scores[index]:.2f}')\n",
    "        y = org[1] + 40\n",
    "        org = (org[0], y)\n",
    "        cv2.putText(video_frame, emotion_str, org, font, font_scale, font_color, thickness, line_type)\n",
    "    \n",
    "# identify Face in Video Stream\n",
    "def detect_bounding_box(video_frame, counter):\n",
    "    global max_emotion\n",
    "    # Check for empty frame (optional, can be handled in evaluate_input())\n",
    "    if video_frame is None:\n",
    "        return None\n",
    "    gray_image = cv2.cvtColor(video_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # notes: MultiScale optimized\n",
    "    faces = face_classifier.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=10, minSize=(64, 64))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(gray_image, (x, y), (x+w, y+h), (255, 0, 0), 0)\n",
    "\n",
    "        # convert the ROI to a dlib rectangle\n",
    "        dlib_rect = dlib.rectangle(x, y, x+w, y+h)\n",
    "\n",
    "        # detect facial landmarks through dlib\n",
    "        landmarks = predictor(gray_image, dlib_rect)\n",
    "\n",
    "        pil_crop_img = video_frame[y : y + h, x : x + w]\n",
    "        rounded_scores, cam = detect_emotion(pil_crop_img)\n",
    "            \n",
    "        if counter == 0:\n",
    "            max_emotion = update_max_emotion(rounded_scores) \n",
    "            \n",
    "        # draw landmarks on the video_frame\n",
    "        for i in range(68):\n",
    "            cv2.circle(video_frame, (landmarks.part(i).x, landmarks.part(i).y), 1, (255, 255, 255), 0)\n",
    "            \n",
    "        plot_heatmap(x, y, w, h, cam, pil_crop_img, video_frame)\n",
    "        print_max_emotion(x, y, max_emotion, video_frame) # displays the max_emotion according to evaluation_frequency\n",
    "        print_all_emotion(x, y, w, rounded_scores, video_frame) # evaluates every video_frame for debugging\n",
    "\n",
    "    return faces\n",
    "\n",
    "def create_video_out(source, input_path_to_video):\n",
    "    if source == 'camera':\n",
    "        video_capture = cv2.VideoCapture(0)\n",
    "        fps = 10\n",
    "        out_file_name = 'cam_eval_video.mp4'\n",
    "    elif source == 'video':\n",
    "        video_capture = cv2.VideoCapture(input_path_to_video)\n",
    "        fps = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
    "        out_file_name = 'eval_video.mp4'\n",
    "    else:\n",
    "        print('unknown input')\n",
    "        print('please enter camera or video')\n",
    "    frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(out_file_name, fourcc, fps, (frame_width, frame_height))\n",
    "    return out, video_capture\n",
    "\n",
    "\n",
    "# loop for Real-Time Face Detection\n",
    "def evaluate_input(source, input_path_to_video):\n",
    "    out, video_capture = create_video_out(source, input_path_to_video)\n",
    "    \n",
    "    counter = 0\n",
    "    evaluation_frequency = 5\n",
    "\n",
    "    while True:\n",
    "\n",
    "        result, video_frame = video_capture.read()  # read frames from the video\n",
    "        if result is False:\n",
    "            break  # terminate the loop if the frame is not read successfully\n",
    "        \n",
    "        faces = detect_bounding_box(video_frame, counter)  # apply the function we created to the video frame, faces as variable not used\n",
    "        \n",
    "        cv2.imshow(\"My Face Detection Project\", video_frame)  # display the processed frame in a window named \"My Face Detection Project\"\n",
    "\n",
    "        out.write(video_frame)  # write the processed frame to the output video file\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "        \n",
    "        counter += 1\n",
    "        if counter == evaluation_frequency:\n",
    "            counter = 0\n",
    "\n",
    "    hook.unregister_hook()\n",
    "    video_capture.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "evaluate_input(source='camera', input_path_to_video=None)\n",
    "#evaluate_input(source='video', input_path_to_video=\"C:/LMU/python/SEP_DLCV/Final_project/last_work/models/demoo.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
